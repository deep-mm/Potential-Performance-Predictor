{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/X_KBest.csv')\n",
    "y = pd.read_csv('data/Y_res.csv')\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2466, 20)\n",
      "(2466, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# import model metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(Y_test, Y_pred):\n",
    "    print('Accuracy Score: ', accuracy_score(Y_test, Y_pred))\n",
    "    print('Confusion Matrix: \\n', confusion_matrix(Y_test, Y_pred))\n",
    "    print('Classification Report: \\n', classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1972\n",
      "Iteration:  1  Error:  0.2707910750507087  Alpha:  0.49530634966040166\n",
      "1972\n",
      "Iteration:  2  Error:  0.5000000000000011  Alpha:  -2.220446049250318e-15\n",
      "1972\n",
      "Iteration:  3  Error:  0.49999999999999417  Alpha:  1.1657341758564008e-14\n",
      "1972\n",
      "Iteration:  4  Error:  0.500000000000029  Alpha:  -5.795364188543653e-14\n",
      "1972\n",
      "Iteration:  5  Error:  0.4999999999999712  Alpha:  5.773159728050481e-14\n",
      "1972\n",
      "Iteration:  6  Error:  0.5000000000000303  Alpha:  -6.061817714453722e-14\n",
      "1972\n",
      "Iteration:  7  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  8  Error:  0.5000000000000303  Alpha:  -6.061817714453722e-14\n",
      "1972\n",
      "Iteration:  9  Error:  0.49999999999997147  Alpha:  5.706546346572979e-14\n",
      "1972\n",
      "Iteration:  10  Error:  0.5000000000000294  Alpha:  -5.884182030513676e-14\n",
      "1972\n",
      "Iteration:  11  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  12  Error:  0.5000000000000303  Alpha:  -6.061817714453722e-14\n",
      "1972\n",
      "Iteration:  13  Error:  0.49999999999997147  Alpha:  5.706546346572979e-14\n",
      "1972\n",
      "Iteration:  14  Error:  0.4999999999999939  Alpha:  1.2212453270876572e-14\n",
      "1972\n",
      "Iteration:  15  Error:  0.500000000000029  Alpha:  -5.795364188543653e-14\n",
      "1972\n",
      "Iteration:  16  Error:  0.4999999999999712  Alpha:  5.773159728050481e-14\n",
      "1972\n",
      "Iteration:  17  Error:  0.5000000000000294  Alpha:  -5.884182030513676e-14\n",
      "1972\n",
      "Iteration:  18  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  19  Error:  0.5000000000000304  Alpha:  -6.084022174946228e-14\n",
      "1972\n",
      "Iteration:  20  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  21  Error:  0.5000000000000303  Alpha:  -6.061817714453722e-14\n",
      "1972\n",
      "Iteration:  22  Error:  0.49999999999997147  Alpha:  5.706546346572979e-14\n",
      "1972\n",
      "Iteration:  23  Error:  0.5000000000000294  Alpha:  -5.884182030513676e-14\n",
      "1972\n",
      "Iteration:  24  Error:  0.49999999999997147  Alpha:  5.706546346572979e-14\n",
      "1972\n",
      "Iteration:  25  Error:  0.49999999999999417  Alpha:  1.1657341758564008e-14\n",
      "1972\n",
      "Iteration:  26  Error:  0.500000000000029  Alpha:  -5.795364188543653e-14\n",
      "1972\n",
      "Iteration:  27  Error:  0.4999999999999712  Alpha:  5.773159728050481e-14\n",
      "1972\n",
      "Iteration:  28  Error:  0.5000000000000294  Alpha:  -5.884182030513676e-14\n",
      "1972\n",
      "Iteration:  29  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  30  Error:  0.5000000000000304  Alpha:  -6.084022174946228e-14\n",
      "1972\n",
      "Iteration:  31  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  32  Error:  0.5000000000000303  Alpha:  -6.061817714453722e-14\n",
      "1972\n",
      "Iteration:  33  Error:  0.49999999999997147  Alpha:  5.706546346572979e-14\n",
      "1972\n",
      "Iteration:  34  Error:  0.5000000000000294  Alpha:  -5.884182030513676e-14\n",
      "1972\n",
      "Iteration:  35  Error:  0.49999999999997147  Alpha:  5.706546346572979e-14\n",
      "1972\n",
      "Iteration:  36  Error:  0.49999999999999417  Alpha:  1.1657341758564008e-14\n",
      "1972\n",
      "Iteration:  37  Error:  0.500000000000029  Alpha:  -5.795364188543653e-14\n",
      "1972\n",
      "Iteration:  38  Error:  0.4999999999999712  Alpha:  5.773159728050481e-14\n",
      "1972\n",
      "Iteration:  39  Error:  0.5000000000000294  Alpha:  -5.884182030513676e-14\n",
      "1972\n",
      "Iteration:  40  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  41  Error:  0.5000000000000304  Alpha:  -6.084022174946228e-14\n",
      "1972\n",
      "Iteration:  42  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  43  Error:  0.5000000000000303  Alpha:  -6.061817714453722e-14\n",
      "1972\n",
      "Iteration:  44  Error:  0.49999999999997147  Alpha:  5.706546346572979e-14\n",
      "1972\n",
      "Iteration:  45  Error:  0.5000000000000294  Alpha:  -5.884182030513676e-14\n",
      "1972\n",
      "Iteration:  46  Error:  0.49999999999997147  Alpha:  5.706546346572979e-14\n",
      "1972\n",
      "Iteration:  47  Error:  0.49999999999999417  Alpha:  1.1657341758564008e-14\n",
      "1972\n",
      "Iteration:  48  Error:  0.500000000000029  Alpha:  -5.795364188543653e-14\n",
      "1972\n",
      "Iteration:  49  Error:  0.4999999999999712  Alpha:  5.773159728050481e-14\n",
      "1972\n",
      "Iteration:  50  Error:  0.5000000000000294  Alpha:  -5.884182030513676e-14\n"
     ]
    }
   ],
   "source": [
    "#AdaBoost Implementation training\n",
    "itr = 50\n",
    "n = len(X_train)\n",
    "w = np.full(n, (1/n))\n",
    "models = []\n",
    "alpha = []\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "for i in range(itr):\n",
    "    model = DecisionTreeClassifier(max_depth=2)\n",
    "    train_data = X_train.sample(n, replace=False, weights=w)\n",
    "    print(len(np.unique(train_data.index, return_counts=False)))\n",
    "    train_label = y_train[train_data.index]\n",
    "    model.fit(train_data, train_label)\n",
    "    Y_pred = model.predict(X_train)\n",
    "    error = 0\n",
    "    for j in range(n):\n",
    "        if Y_pred[j] != y_train[j]:\n",
    "            error += w[j]\n",
    "    alpha.append(0.5 * np.log((1-error)/error))\n",
    "    for j in range(n):\n",
    "        if Y_pred[j] == y_train[j]:\n",
    "            w[j] = w[j] * np.exp(-alpha[i])\n",
    "        else:\n",
    "            w[j] = w[j] * np.exp(alpha[i])\n",
    "    w = w/sum(w)\n",
    "    models.append(model)\n",
    "    print('Iteration: ', i+1, ' Error: ', error, ' Alpha: ', alpha[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.7307692307692307\n",
      "Confusion Matrix: \n",
      " [[182  63]\n",
      " [ 70 179]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73       245\n",
      "           1       0.74      0.72      0.73       249\n",
      "\n",
      "    accuracy                           0.73       494\n",
      "   macro avg       0.73      0.73      0.73       494\n",
      "weighted avg       0.73      0.73      0.73       494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing AdaBoost\n",
    "t_n = len(X_test)\n",
    "Y_pred = []\n",
    "for i in range(t_n):\n",
    "    pred = 0\n",
    "    for m in range(len(models)):\n",
    "        pred += alpha[m] * models[m].predict([X_test.iloc[i]])\n",
    "    if pred > 0:\n",
    "        Y_pred.append(1)\n",
    "    else:\n",
    "        Y_pred.append(0)\n",
    "\n",
    "print_report(y_test, Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8522267206477733\n",
      "Confusion Matrix: \n",
      " [[212  33]\n",
      " [ 40 209]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.85       245\n",
      "           1       0.86      0.84      0.85       249\n",
      "\n",
      "    accuracy                           0.85       494\n",
      "   macro avg       0.85      0.85      0.85       494\n",
      "weighted avg       0.85      0.85      0.85       494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=50)\n",
    "model.fit(X_train, y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "print_report(y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model, X_train, y_train, w, samples_len):\n",
    "    train_data = X_train.sample(samples_len, replace=False, weights=w)\n",
    "    train_label = y_train[train_data.index]\n",
    "    model.fit(train_data, train_label)\n",
    "    Y_pred = model.predict(X_train)\n",
    "    error = 0\n",
    "    for j in range(len(X_train)):\n",
    "        if Y_pred[j] != y_train[j]:\n",
    "            error += w[j]\n",
    "\n",
    "    w_temp = w.copy()\n",
    "    if error != 0:\n",
    "        alpha = 0.5 * np.log((1-error)/error)\n",
    "        for j in range(n):\n",
    "            if Y_pred[j] == y_train[j]:\n",
    "                w_temp[j] = w[j] * np.exp(-alpha)\n",
    "            else:\n",
    "                w_temp[j] = w[j] * np.exp(alpha)\n",
    "    return model, error, w_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Regression Accuracy:  0.5694726166328601\n",
      "KNN Accuracy:  0.8012170385395537\n",
      "Decision Tree Accuracy:  0.6952332657200812\n",
      "MLP Accuracy:  0.597870182555781\n",
      "SVM Accuracy:  0.7931034482758621\n",
      "Random Forest Accuracy:  0.7505070993914807\n",
      "AdaBoost Accuracy:  0.9061866125760649\n",
      "Iteration:  1 Weighted Error:  0.2694871051869029\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5775862068965517\n",
      "KNN Accuracy:  0.7799188640973631\n",
      "Decision Tree Accuracy:  0.6825557809330629\n",
      "MLP Accuracy:  0.5953346855983773\n",
      "SVM Accuracy:  0.8037525354969574\n",
      "Random Forest Accuracy:  0.7697768762677485\n",
      "AdaBoost Accuracy:  0.904158215010142\n",
      "Iteration:  2 Weighted Error:  0.3161737043855784\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.563894523326572\n",
      "KNN Accuracy:  0.7915821501014199\n",
      "Decision Tree Accuracy:  0.6926977687626775\n",
      "MLP Accuracy:  0.5983772819472617\n",
      "SVM Accuracy:  0.8037525354969574\n",
      "Random Forest Accuracy:  0.7692697768762677\n",
      "AdaBoost Accuracy:  0.9132860040567952\n",
      "Iteration:  3 Weighted Error:  0.3389871096375841\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5968559837728195\n",
      "KNN Accuracy:  0.7865111561866126\n",
      "Decision Tree Accuracy:  0.72920892494929\n",
      "MLP Accuracy:  0.5912778904665315\n",
      "SVM Accuracy:  0.7915821501014199\n",
      "Random Forest Accuracy:  0.7814401622718052\n",
      "AdaBoost Accuracy:  0.9127789046653144\n",
      "Iteration:  4 Weighted Error:  0.3541032597208062\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5780933062880325\n",
      "KNN Accuracy:  0.795131845841785\n",
      "Decision Tree Accuracy:  0.6607505070993914\n",
      "MLP Accuracy:  0.5988843813387424\n",
      "SVM Accuracy:  0.7986815415821501\n",
      "Random Forest Accuracy:  0.8154158215010142\n",
      "AdaBoost Accuracy:  0.902129817444219\n",
      "Iteration:  5 Weighted Error:  0.37308835898663284\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5841784989858012\n",
      "KNN Accuracy:  0.7976673427991886\n",
      "Decision Tree Accuracy:  0.72920892494929\n",
      "MLP Accuracy:  0.5557809330628803\n",
      "SVM Accuracy:  0.7966531440162272\n",
      "Random Forest Accuracy:  0.7662271805273834\n",
      "AdaBoost Accuracy:  0.9127789046653144\n",
      "Iteration:  6 Weighted Error:  0.3941813134495673\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5015212981744422\n",
      "KNN Accuracy:  0.7900608519269777\n",
      "Decision Tree Accuracy:  0.72920892494929\n",
      "MLP Accuracy:  0.5765720081135902\n",
      "SVM Accuracy:  0.787525354969574\n",
      "Random Forest Accuracy:  0.7849898580121704\n",
      "AdaBoost Accuracy:  0.9117647058823529\n",
      "Iteration:  7 Weighted Error:  0.4025458390857587\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5735294117647058\n",
      "KNN Accuracy:  0.7829614604462475\n",
      "Decision Tree Accuracy:  0.6962474645030426\n",
      "MLP Accuracy:  0.507606490872211\n",
      "SVM Accuracy:  0.7996957403651116\n",
      "Random Forest Accuracy:  0.802738336713996\n",
      "AdaBoost Accuracy:  0.9112576064908722\n",
      "Iteration:  8 Weighted Error:  0.4024915720918427\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5836713995943205\n",
      "KNN Accuracy:  0.7905679513184585\n",
      "Decision Tree Accuracy:  0.72920892494929\n",
      "MLP Accuracy:  0.5141987829614605\n",
      "SVM Accuracy:  0.7880324543610547\n",
      "Random Forest Accuracy:  0.768762677484787\n",
      "AdaBoost Accuracy:  0.9016227180527383\n",
      "Iteration:  9 Weighted Error:  0.4212826291589859\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5882352941176471\n",
      "KNN Accuracy:  0.7996957403651116\n",
      "Decision Tree Accuracy:  0.6926977687626775\n",
      "MLP Accuracy:  0.5172413793103449\n",
      "SVM Accuracy:  0.7885395537525355\n",
      "Random Forest Accuracy:  0.7596348884381339\n",
      "AdaBoost Accuracy:  0.915314401622718\n",
      "Iteration:  10 Weighted Error:  0.41411311467052714\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5370182555780934\n",
      "KNN Accuracy:  0.8037525354969574\n",
      "Decision Tree Accuracy:  0.6485801217038539\n",
      "MLP Accuracy:  0.5070993914807302\n",
      "SVM Accuracy:  0.7991886409736308\n",
      "Random Forest Accuracy:  0.77079107505071\n",
      "AdaBoost Accuracy:  0.9188640973630832\n",
      "Iteration:  11 Weighted Error:  0.4099257472561194\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iter = 11\n",
    "n = len(X_train)\n",
    "samples_len = n//2\n",
    "w = np.full(n, (1/n))\n",
    "models = []\n",
    "alpha = []\n",
    "n1 = 1/n\n",
    "\n",
    "for i in range(iter):\n",
    "    model1, error1, w1 = create_model(LogisticRegression(C=0.1, max_iter=10, penalty='l2'), X_train, y_train, w, samples_len)\n",
    "    model2, error2, w2 = create_model(KNeighborsClassifier(algorithm='auto', n_neighbors=3, weights='uniform'), X_train, y_train, w, samples_len)\n",
    "    model3, error3, w3 = create_model(DecisionTreeClassifier(criterion='entropy', max_depth=2, splitter='best'), X_train, y_train, w, samples_len)\n",
    "    model4, error4, w4 = create_model(MLPClassifier(activation='tanh', alpha=0.05, hidden_layer_sizes=(20, 30, 30, 20), learning_rate='adaptive', solver='adam'), X_train, y_train, w, samples_len)\n",
    "    model5, error5, w5 = create_model(SVC(C=10, gamma=0.1, kernel='linear'), X_train, y_train, w, samples_len)\n",
    "    model6, error6, w6 = create_model(RandomForestClassifier(criterion='entropy', max_depth=2, n_estimators=10), X_train, y_train, w, samples_len)\n",
    "    model7, error7, w7 = create_model(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=50), X_train, y_train, w, samples_len)\n",
    "\n",
    "    error = error1 + error2 + error3 + error4 + error5 + error6 + error7\n",
    "    error = error/7\n",
    "    alpha_round = 0.5 * np.log((1-error)/error)\n",
    "    w = w1 + w2 + w3 + w4 + w5 + w6 + w7\n",
    "    w = w/sum(w)\n",
    "\n",
    "    models.append([model1, model2, model3, model4, model5, model6, model7])\n",
    "\n",
    "    print('Login Regression Accuracy: ', accuracy_score(y_train, model1.predict(X_train)))\n",
    "    print('KNN Accuracy: ', accuracy_score(y_train, model2.predict(X_train)))\n",
    "    print('Decision Tree Accuracy: ', accuracy_score(y_train, model3.predict(X_train)))\n",
    "    print('MLP Accuracy: ', accuracy_score(y_train, model4.predict(X_train)))\n",
    "    print('SVM Accuracy: ', accuracy_score(y_train, model5.predict(X_train)))\n",
    "    print('Random Forest Accuracy: ', accuracy_score(y_train, model6.predict(X_train)))\n",
    "    print('AdaBoost Accuracy: ', accuracy_score(y_train, model7.predict(X_train)))\n",
    "    print('Iteration: ', i+1, 'Weighted Error: ', error)\n",
    "    print('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save models and alpha\n",
    "import pickle\n",
    "with open('models/models.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)\n",
    "with open('models/alpha.pkl', 'wb') as f:\n",
    "    pickle.dump(alpha, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load models and alpha\n",
    "# import pickle\n",
    "# with open('models/models.pkl', 'rb') as f:\n",
    "#     models = pickle.load(f)\n",
    "\n",
    "# with open('models/alpha.pkl', 'rb') as f:\n",
    "#     alpha = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(x, models):\n",
    "    pred = []\n",
    "    for i in range(len(models)):\n",
    "        pred.append(models[i].predict(x))\n",
    "    pred = np.array(pred)\n",
    "    u, c = np.unique(pred, return_counts=True)\n",
    "    f_pred = u[np.argmax(c)]\n",
    "    return f_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8421052631578947\n",
      "Confusion Matrix: \n",
      " [[218  27]\n",
      " [ 51 198]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.89      0.85       245\n",
      "           1       0.88      0.80      0.84       249\n",
      "\n",
      "    accuracy                           0.84       494\n",
      "   macro avg       0.85      0.84      0.84       494\n",
      "weighted avg       0.85      0.84      0.84       494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_n = len(X_test)\n",
    "Y_pred = np.zeros(test_n)\n",
    "for i in range(test_n):\n",
    "    x = X_test.iloc[i].values.reshape(1, -1)\n",
    "    temp = []\n",
    "    for m in range(len(models)):\n",
    "        temp.append(get_pred(x, models[m]))\n",
    "\n",
    "    u, c = np.unique(temp, return_counts=True)\n",
    "    Y_pred[i] = u[np.argmax(c)]\n",
    "\n",
    "print_report(y_test, Y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unbiased Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/X_Unbiased.csv')\n",
    "y = pd.read_csv('data/Y_Unbiased.csv')\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Regression Accuracy:  0.5806288032454361\n",
      "KNN Accuracy:  0.7956389452332657\n",
      "Decision Tree Accuracy:  0.6906693711967545\n",
      "MLP Accuracy:  0.5765720081135902\n",
      "SVM Accuracy:  0.795131845841785\n",
      "Random Forest Accuracy:  0.7692697768762677\n",
      "AdaBoost Accuracy:  0.8914807302231237\n",
      "Iteration:  1 Weighted Error:  0.271515502752826\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.59026369168357\n",
      "KNN Accuracy:  0.8067951318458418\n",
      "Decision Tree Accuracy:  0.6825557809330629\n",
      "MLP Accuracy:  0.5872210953346856\n",
      "SVM Accuracy:  0.8088235294117647\n",
      "Random Forest Accuracy:  0.7738336713995944\n",
      "AdaBoost Accuracy:  0.9046653144016227\n",
      "Iteration:  2 Weighted Error:  0.29933954163403415\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5856997971602435\n",
      "KNN Accuracy:  0.8057809330628803\n",
      "Decision Tree Accuracy:  0.72920892494929\n",
      "MLP Accuracy:  0.603448275862069\n",
      "SVM Accuracy:  0.8012170385395537\n",
      "Random Forest Accuracy:  0.7205882352941176\n",
      "AdaBoost Accuracy:  0.8914807302231237\n",
      "Iteration:  3 Weighted Error:  0.34297182166960655\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5562880324543611\n",
      "KNN Accuracy:  0.7834685598377282\n",
      "Decision Tree Accuracy:  0.7337728194726166\n",
      "MLP Accuracy:  0.5730223123732252\n",
      "SVM Accuracy:  0.7697768762677485\n",
      "Random Forest Accuracy:  0.7783975659229209\n",
      "AdaBoost Accuracy:  0.8884381338742393\n",
      "Iteration:  4 Weighted Error:  0.3679681069246504\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5512170385395537\n",
      "KNN Accuracy:  0.7824543610547667\n",
      "Decision Tree Accuracy:  0.6724137931034483\n",
      "MLP Accuracy:  0.5141987829614605\n",
      "SVM Accuracy:  0.7936105476673428\n",
      "Random Forest Accuracy:  0.7702839756592292\n",
      "AdaBoost Accuracy:  0.8955375253549696\n",
      "Iteration:  5 Weighted Error:  0.37555356258138745\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5907707910750507\n",
      "KNN Accuracy:  0.787525354969574\n",
      "Decision Tree Accuracy:  0.6926977687626775\n",
      "MLP Accuracy:  0.571501014198783\n",
      "SVM Accuracy:  0.8022312373225152\n",
      "Random Forest Accuracy:  0.7550709939148073\n",
      "AdaBoost Accuracy:  0.9117647058823529\n",
      "Iteration:  6 Weighted Error:  0.39872640302935636\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5765720081135902\n",
      "KNN Accuracy:  0.7915821501014199\n",
      "Decision Tree Accuracy:  0.723630831643002\n",
      "MLP Accuracy:  0.5268762677484787\n",
      "SVM Accuracy:  0.8017241379310345\n",
      "Random Forest Accuracy:  0.7535496957403651\n",
      "AdaBoost Accuracy:  0.8950304259634888\n",
      "Iteration:  7 Weighted Error:  0.4043054791217458\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5765720081135902\n",
      "KNN Accuracy:  0.7920892494929006\n",
      "Decision Tree Accuracy:  0.6926977687626775\n",
      "MLP Accuracy:  0.5314401622718052\n",
      "SVM Accuracy:  0.8113590263691683\n",
      "Random Forest Accuracy:  0.7768762677484787\n",
      "AdaBoost Accuracy:  0.8879310344827587\n",
      "Iteration:  8 Weighted Error:  0.41787013047933097\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5765720081135902\n",
      "KNN Accuracy:  0.7890466531440162\n",
      "Decision Tree Accuracy:  0.72920892494929\n",
      "MLP Accuracy:  0.5786004056795132\n",
      "SVM Accuracy:  0.8032454361054767\n",
      "Random Forest Accuracy:  0.7849898580121704\n",
      "AdaBoost Accuracy:  0.8930020283975659\n",
      "Iteration:  9 Weighted Error:  0.42978827302518047\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5491886409736308\n",
      "KNN Accuracy:  0.781947261663286\n",
      "Decision Tree Accuracy:  0.6135902636916836\n",
      "MLP Accuracy:  0.6029411764705882\n",
      "SVM Accuracy:  0.7946247464503042\n",
      "Random Forest Accuracy:  0.7439148073022313\n",
      "AdaBoost Accuracy:  0.8975659229208925\n",
      "Iteration:  10 Weighted Error:  0.4413752613801664\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5786004056795132\n",
      "KNN Accuracy:  0.8067951318458418\n",
      "Decision Tree Accuracy:  0.6135902636916836\n",
      "MLP Accuracy:  0.5765720081135902\n",
      "SVM Accuracy:  0.781947261663286\n",
      "Random Forest Accuracy:  0.7373225152129818\n",
      "AdaBoost Accuracy:  0.9016227180527383\n",
      "Iteration:  11 Weighted Error:  0.45503850657582784\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iter = 11\n",
    "n = len(X_train)\n",
    "samples_len = n//2\n",
    "w = np.full(n, (1/n))\n",
    "models = []\n",
    "alpha = []\n",
    "n1 = 1/n\n",
    "\n",
    "for i in range(iter):\n",
    "    model1, error1, w1 = create_model(LogisticRegression(C=0.1, max_iter=10, penalty='l2'), X_train, y_train, w, samples_len)\n",
    "    model2, error2, w2 = create_model(KNeighborsClassifier(algorithm='auto', n_neighbors=3, weights='uniform'), X_train, y_train, w, samples_len)\n",
    "    model3, error3, w3 = create_model(DecisionTreeClassifier(criterion='entropy', max_depth=2, splitter='best'), X_train, y_train, w, samples_len)\n",
    "    model4, error4, w4 = create_model(MLPClassifier(activation='tanh', alpha=0.05, hidden_layer_sizes=(20, 30, 30, 20), learning_rate='adaptive', solver='adam'), X_train, y_train, w, samples_len)\n",
    "    model5, error5, w5 = create_model(SVC(C=10, gamma=0.1, kernel='linear'), X_train, y_train, w, samples_len)\n",
    "    model6, error6, w6 = create_model(RandomForestClassifier(criterion='entropy', max_depth=2, n_estimators=10), X_train, y_train, w, samples_len)\n",
    "    model7, error7, w7 = create_model(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=50), X_train, y_train, w, samples_len)\n",
    "\n",
    "    error = error1 + error2 + error3 + error4 + error5 + error6 + error7\n",
    "    error = error/7\n",
    "    alpha_round = 0.5 * np.log((1-error)/error)\n",
    "    w = w1 + w2 + w3 + w4 + w5 + w6 + w7\n",
    "    w = w/sum(w)\n",
    "\n",
    "\n",
    "    models.append([model1, model2, model3, model4, model5, model6, model7])\n",
    "    print('Login Regression Accuracy: ', accuracy_score(y_train, model1.predict(X_train)))\n",
    "    print('KNN Accuracy: ', accuracy_score(y_train, model2.predict(X_train)))\n",
    "    print('Decision Tree Accuracy: ', accuracy_score(y_train, model3.predict(X_train)))\n",
    "    print('MLP Accuracy: ', accuracy_score(y_train, model4.predict(X_train)))\n",
    "    print('SVM Accuracy: ', accuracy_score(y_train, model5.predict(X_train)))\n",
    "    print('Random Forest Accuracy: ', accuracy_score(y_train, model6.predict(X_train)))\n",
    "    print('AdaBoost Accuracy: ', accuracy_score(y_train, model7.predict(X_train)))\n",
    "    print('Iteration: ', i+1, 'Weighted Error: ', error)\n",
    "    print('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save models and alpha\n",
    "import pickle\n",
    "with open('models/models_unbiased.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)\n",
    "with open('models/alpha_unbiased.pkl', 'wb') as f:\n",
    "    pickle.dump(alpha, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8340080971659919\n",
      "Confusion Matrix: \n",
      " [[207  38]\n",
      " [ 44 205]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       245\n",
      "           1       0.84      0.82      0.83       249\n",
      "\n",
      "    accuracy                           0.83       494\n",
      "   macro avg       0.83      0.83      0.83       494\n",
      "weighted avg       0.83      0.83      0.83       494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_n = len(X_test)\n",
    "Y_pred = np.zeros(test_n)\n",
    "for i in range(test_n):\n",
    "    x = X_test.iloc[i].values.reshape(1, -1)\n",
    "    temp = []\n",
    "    for m in range(len(models)):\n",
    "        temp.append(get_pred(x, models[m]))\n",
    "\n",
    "    u, c = np.unique(temp, return_counts=True)\n",
    "    Y_pred[i] = u[np.argmax(c)]\n",
    "\n",
    "print_report(y_test, Y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
