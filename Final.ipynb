{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/X_KBest.csv')\n",
    "y = pd.read_csv('data/Y_res.csv')\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2466, 20)\n",
      "(2466, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# import model metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(Y_test, Y_pred):\n",
    "    print('Accuracy Score: ', accuracy_score(Y_test, Y_pred))\n",
    "    print('Confusion Matrix: \\n', confusion_matrix(Y_test, Y_pred))\n",
    "    print('Classification Report: \\n', classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1972\n",
      "Iteration:  1  Error:  0.2707910750507087  Alpha:  0.49530634966040166\n",
      "1972\n",
      "Iteration:  2  Error:  0.5000000000000011  Alpha:  -2.2204460492503182e-15\n",
      "1972\n",
      "Iteration:  3  Error:  0.49999999999999417  Alpha:  1.1657341758564008e-14\n",
      "1972\n",
      "Iteration:  4  Error:  0.500000000000029  Alpha:  -5.795364188543653e-14\n",
      "1972\n",
      "Iteration:  5  Error:  0.4999999999999712  Alpha:  5.773159728050481e-14\n",
      "1972\n",
      "Iteration:  6  Error:  0.5000000000000303  Alpha:  -6.061817714453722e-14\n",
      "1972\n",
      "Iteration:  7  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  8  Error:  0.5000000000000303  Alpha:  -6.061817714453722e-14\n",
      "1972\n",
      "Iteration:  9  Error:  0.49999999999997147  Alpha:  5.706546346572979e-14\n",
      "1972\n",
      "Iteration:  10  Error:  0.5000000000000294  Alpha:  -5.884182030513676e-14\n",
      "1972\n",
      "Iteration:  11  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  12  Error:  0.5000000000000294  Alpha:  -5.884182030513676e-14\n",
      "1972\n",
      "Iteration:  13  Error:  0.49999999999997147  Alpha:  5.706546346572979e-14\n",
      "1972\n",
      "Iteration:  14  Error:  0.49999999999999417  Alpha:  1.1657341758564008e-14\n",
      "1972\n",
      "Iteration:  15  Error:  0.500000000000029  Alpha:  -5.795364188543653e-14\n",
      "1972\n",
      "Iteration:  16  Error:  0.4999999999999712  Alpha:  5.773159728050481e-14\n",
      "1972\n",
      "Iteration:  17  Error:  0.5000000000000303  Alpha:  -6.061817714453722e-14\n",
      "1972\n",
      "Iteration:  18  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  19  Error:  0.5000000000000303  Alpha:  -6.061817714453722e-14\n",
      "1972\n",
      "Iteration:  20  Error:  0.49999999999997147  Alpha:  5.706546346572979e-14\n",
      "1972\n",
      "Iteration:  21  Error:  0.5000000000000294  Alpha:  -5.884182030513676e-14\n",
      "1972\n",
      "Iteration:  22  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  23  Error:  0.5000000000000303  Alpha:  -6.061817714453722e-14\n",
      "1972\n",
      "Iteration:  24  Error:  0.49999999999997147  Alpha:  5.706546346572979e-14\n",
      "1972\n",
      "Iteration:  25  Error:  0.49999999999999395  Alpha:  1.210143096841406e-14\n",
      "1972\n",
      "Iteration:  26  Error:  0.500000000000029  Alpha:  -5.795364188543653e-14\n",
      "1972\n",
      "Iteration:  27  Error:  0.4999999999999712  Alpha:  5.773159728050481e-14\n",
      "1972\n",
      "Iteration:  28  Error:  0.5000000000000294  Alpha:  -5.884182030513676e-14\n",
      "1972\n",
      "Iteration:  29  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  30  Error:  0.5000000000000304  Alpha:  -6.084022174946228e-14\n",
      "1972\n",
      "Iteration:  31  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  32  Error:  0.5000000000000294  Alpha:  -5.884182030513676e-14\n",
      "1972\n",
      "Iteration:  33  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  34  Error:  0.5000000000000303  Alpha:  -6.061817714453722e-14\n",
      "1972\n",
      "Iteration:  35  Error:  0.49999999999997147  Alpha:  5.706546346572979e-14\n",
      "1972\n",
      "Iteration:  36  Error:  0.49999999999999395  Alpha:  1.210143096841406e-14\n",
      "1972\n",
      "Iteration:  37  Error:  0.500000000000029  Alpha:  -5.795364188543653e-14\n",
      "1972\n",
      "Iteration:  38  Error:  0.4999999999999712  Alpha:  5.773159728050481e-14\n",
      "1972\n",
      "Iteration:  39  Error:  0.5000000000000294  Alpha:  -5.884182030513676e-14\n",
      "1972\n",
      "Iteration:  40  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  41  Error:  0.5000000000000304  Alpha:  -6.084022174946228e-14\n",
      "1972\n",
      "Iteration:  42  Error:  0.4999999999999715  Alpha:  5.695444116326729e-14\n",
      "1972\n",
      "Iteration:  43  Error:  0.5000000000000303  Alpha:  -6.061817714453722e-14\n",
      "1972\n",
      "Iteration:  44  Error:  0.49999999999997147  Alpha:  5.706546346572979e-14\n",
      "1972\n",
      "Iteration:  45  Error:  0.5000000000000294  Alpha:  -5.884182030513676e-14\n",
      "1972\n",
      "Iteration:  46  Error:  0.49999999999997147  Alpha:  5.706546346572979e-14\n",
      "1972\n",
      "Iteration:  47  Error:  0.49999999999999417  Alpha:  1.1657341758564008e-14\n",
      "1972\n",
      "Iteration:  48  Error:  0.500000000000029  Alpha:  -5.795364188543653e-14\n",
      "1972\n",
      "Iteration:  49  Error:  0.4999999999999712  Alpha:  5.773159728050481e-14\n",
      "1972\n",
      "Iteration:  50  Error:  0.5000000000000303  Alpha:  -6.061817714453722e-14\n"
     ]
    }
   ],
   "source": [
    "#AdaBoost Implementation training\n",
    "itr = 50\n",
    "n = len(X_train)\n",
    "w = np.full(n, (1/n))\n",
    "models = []\n",
    "alpha = []\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "for i in range(itr):\n",
    "    model = DecisionTreeClassifier(max_depth=2)\n",
    "    train_data = X_train.sample(n, replace=False, weights=w)\n",
    "    print(len(np.unique(train_data.index, return_counts=False)))\n",
    "    train_label = y_train[train_data.index]\n",
    "    model.fit(train_data, train_label)\n",
    "    Y_pred = model.predict(X_train)\n",
    "    error = 0\n",
    "    for j in range(n):\n",
    "        if Y_pred[j] != y_train[j]:\n",
    "            error += w[j]\n",
    "    alpha.append(0.5 * np.log((1-error)/error))\n",
    "    for j in range(n):\n",
    "        if Y_pred[j] == y_train[j]:\n",
    "            w[j] = w[j] * np.exp(-alpha[i])\n",
    "        else:\n",
    "            w[j] = w[j] * np.exp(alpha[i])\n",
    "    w = w/sum(w)\n",
    "    models.append(model)\n",
    "    print('Iteration: ', i+1, ' Error: ', error, ' Alpha: ', alpha[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.7307692307692307\n",
      "Confusion Matrix: \n",
      " [[182  63]\n",
      " [ 70 179]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73       245\n",
      "           1       0.74      0.72      0.73       249\n",
      "\n",
      "    accuracy                           0.73       494\n",
      "   macro avg       0.73      0.73      0.73       494\n",
      "weighted avg       0.73      0.73      0.73       494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing AdaBoost\n",
    "t_n = len(X_test)\n",
    "Y_pred = []\n",
    "for i in range(t_n):\n",
    "    pred = 0\n",
    "    for m in range(len(models)):\n",
    "        pred += alpha[m] * models[m].predict([X_test.iloc[i]])\n",
    "    if pred > 0:\n",
    "        Y_pred.append(1)\n",
    "    else:\n",
    "        Y_pred.append(0)\n",
    "\n",
    "print_report(y_test, Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8502024291497976\n",
      "Confusion Matrix: \n",
      " [[211  34]\n",
      " [ 40 209]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85       245\n",
      "           1       0.86      0.84      0.85       249\n",
      "\n",
      "    accuracy                           0.85       494\n",
      "   macro avg       0.85      0.85      0.85       494\n",
      "weighted avg       0.85      0.85      0.85       494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=50)\n",
    "model.fit(X_train, y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "print_report(y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model, X_train, y_train, w, samples_len):\n",
    "    train_data = X_train.sample(samples_len, replace=False, weights=w)\n",
    "    train_label = y_train[train_data.index]\n",
    "    model.fit(train_data, train_label)\n",
    "    Y_pred = model.predict(X_train)\n",
    "    error = 0\n",
    "    for j in range(len(X_train)):\n",
    "        if Y_pred[j] != y_train[j]:\n",
    "            error += w[j]\n",
    "\n",
    "    w_temp = w.copy()\n",
    "    if error != 0:\n",
    "        alpha = 0.5 * np.log((1-error)/error)\n",
    "        for j in range(n):\n",
    "            if Y_pred[j] == y_train[j]:\n",
    "                w_temp[j] = w[j] * np.exp(-alpha)\n",
    "            else:\n",
    "                w_temp[j] = w[j] * np.exp(alpha)\n",
    "    return model, error, w_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Regression Accuracy:  0.579107505070994\n",
      "KNN Accuracy:  0.7956389452332657\n",
      "Decision Tree Accuracy:  0.723630831643002\n",
      "MLP Accuracy:  0.5831643002028397\n",
      "SVM Accuracy:  0.800709939148073\n",
      "Random Forest Accuracy:  0.7601419878296146\n",
      "AdaBoost Accuracy:  0.9143002028397565\n",
      "Iteration:  1 Weighted Error:  0.2633294697189225\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5704868154158215\n",
      "KNN Accuracy:  0.7834685598377282\n",
      "Decision Tree Accuracy:  0.72920892494929\n",
      "MLP Accuracy:  0.5983772819472617\n",
      "SVM Accuracy:  0.7931034482758621\n",
      "Random Forest Accuracy:  0.7723123732251521\n",
      "AdaBoost Accuracy:  0.8985801217038539\n",
      "Iteration:  2 Weighted Error:  0.3096642011737805\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5831643002028397\n",
      "KNN Accuracy:  0.7900608519269777\n",
      "Decision Tree Accuracy:  0.6926977687626775\n",
      "MLP Accuracy:  0.5253549695740365\n",
      "SVM Accuracy:  0.7925963488843814\n",
      "Random Forest Accuracy:  0.7814401622718052\n",
      "AdaBoost Accuracy:  0.907707910750507\n",
      "Iteration:  3 Weighted Error:  0.3374448953443154\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5689655172413793\n",
      "KNN Accuracy:  0.7915821501014199\n",
      "Decision Tree Accuracy:  0.72920892494929\n",
      "MLP Accuracy:  0.571501014198783\n",
      "SVM Accuracy:  0.8113590263691683\n",
      "Random Forest Accuracy:  0.7464503042596349\n",
      "AdaBoost Accuracy:  0.8975659229208925\n",
      "Iteration:  4 Weighted Error:  0.36280973855385945\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5821501014198783\n",
      "KNN Accuracy:  0.7920892494929006\n",
      "Decision Tree Accuracy:  0.72920892494929\n",
      "MLP Accuracy:  0.6115618661257607\n",
      "SVM Accuracy:  0.8103448275862069\n",
      "Random Forest Accuracy:  0.7865111561866126\n",
      "AdaBoost Accuracy:  0.9082150101419878\n",
      "Iteration:  5 Weighted Error:  0.37259729843216755\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5841784989858012\n",
      "KNN Accuracy:  0.7971602434077079\n",
      "Decision Tree Accuracy:  0.7119675456389453\n",
      "MLP Accuracy:  0.6105476673427992\n",
      "SVM Accuracy:  0.815922920892495\n",
      "Random Forest Accuracy:  0.7814401622718052\n",
      "AdaBoost Accuracy:  0.9102434077079108\n",
      "Iteration:  6 Weighted Error:  0.3894706581909206\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5998985801217038\n",
      "KNN Accuracy:  0.7971602434077079\n",
      "Decision Tree Accuracy:  0.6926977687626775\n",
      "MLP Accuracy:  0.6217038539553753\n",
      "SVM Accuracy:  0.7880324543610547\n",
      "Random Forest Accuracy:  0.7961460446247465\n",
      "AdaBoost Accuracy:  0.9026369168356998\n",
      "Iteration:  7 Weighted Error:  0.4048411024856356\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5628803245436106\n",
      "KNN Accuracy:  0.7931034482758621\n",
      "Decision Tree Accuracy:  0.72920892494929\n",
      "MLP Accuracy:  0.577079107505071\n",
      "SVM Accuracy:  0.8012170385395537\n",
      "Random Forest Accuracy:  0.7520283975659229\n",
      "AdaBoost Accuracy:  0.9132860040567952\n",
      "Iteration:  8 Weighted Error:  0.4151763276352388\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5603448275862069\n",
      "KNN Accuracy:  0.8052738336713996\n",
      "Decision Tree Accuracy:  0.6724137931034483\n",
      "MLP Accuracy:  0.5740365111561866\n",
      "SVM Accuracy:  0.800709939148073\n",
      "Random Forest Accuracy:  0.7363083164300203\n",
      "AdaBoost Accuracy:  0.9092292089249493\n",
      "Iteration:  9 Weighted Error:  0.4243519626593374\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5010141987829615\n",
      "KNN Accuracy:  0.7976673427991886\n",
      "Decision Tree Accuracy:  0.6926977687626775\n",
      "MLP Accuracy:  0.6019269776876268\n",
      "SVM Accuracy:  0.802738336713996\n",
      "Random Forest Accuracy:  0.7677484787018256\n",
      "AdaBoost Accuracy:  0.9072008113590264\n",
      "Iteration:  10 Weighted Error:  0.4394895203457624\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5887423935091278\n",
      "KNN Accuracy:  0.787525354969574\n",
      "Decision Tree Accuracy:  0.6404665314401623\n",
      "MLP Accuracy:  0.5547667342799188\n",
      "SVM Accuracy:  0.7778904665314401\n",
      "Random Forest Accuracy:  0.7910750507099391\n",
      "AdaBoost Accuracy:  0.8899594320486816\n",
      "Iteration:  11 Weighted Error:  0.43916298537158627\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iter = 11\n",
    "n = len(X_train)\n",
    "samples_len = n//2\n",
    "w = np.full(n, (1/n))\n",
    "models = []\n",
    "alpha = []\n",
    "n1 = 1/n\n",
    "\n",
    "for i in range(iter):\n",
    "    model1, error1, w1 = create_model(LogisticRegression(C=0.1, max_iter=10, penalty='l2'), X_train, y_train, w, samples_len)\n",
    "    model2, error2, w2 = create_model(KNeighborsClassifier(algorithm='auto', n_neighbors=3, weights='uniform'), X_train, y_train, w, samples_len)\n",
    "    model3, error3, w3 = create_model(DecisionTreeClassifier(criterion='entropy', max_depth=2, splitter='best'), X_train, y_train, w, samples_len)\n",
    "    model4, error4, w4 = create_model(MLPClassifier(activation='tanh', alpha=0.05, hidden_layer_sizes=(20, 30, 30, 20), learning_rate='adaptive', solver='adam'), X_train, y_train, w, samples_len)\n",
    "    model5, error5, w5 = create_model(SVC(C=10, gamma=0.1, kernel='linear'), X_train, y_train, w, samples_len)\n",
    "    model6, error6, w6 = create_model(RandomForestClassifier(criterion='entropy', max_depth=2, n_estimators=10), X_train, y_train, w, samples_len)\n",
    "    model7, error7, w7 = create_model(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=50), X_train, y_train, w, samples_len)\n",
    "\n",
    "    error = error1 + error2 + error3 + error4 + error5 + error6 + error7\n",
    "    error = error/7\n",
    "    alpha_round = 0.5 * np.log((1-error)/error)\n",
    "    w = w1 + w2 + w3 + w4 + w5 + w6 + w7\n",
    "    w = w/sum(w)\n",
    "\n",
    "    models.append([model1, model2, model3, model4, model5, model6, model7])\n",
    "\n",
    "    print('Login Regression Accuracy: ', accuracy_score(y_train, model1.predict(X_train)))\n",
    "    print('KNN Accuracy: ', accuracy_score(y_train, model2.predict(X_train)))\n",
    "    print('Decision Tree Accuracy: ', accuracy_score(y_train, model3.predict(X_train)))\n",
    "    print('MLP Accuracy: ', accuracy_score(y_train, model4.predict(X_train)))\n",
    "    print('SVM Accuracy: ', accuracy_score(y_train, model5.predict(X_train)))\n",
    "    print('Random Forest Accuracy: ', accuracy_score(y_train, model6.predict(X_train)))\n",
    "    print('AdaBoost Accuracy: ', accuracy_score(y_train, model7.predict(X_train)))\n",
    "    print('Iteration: ', i+1, 'Weighted Error: ', error)\n",
    "    print('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Every model for each iteration accua\n",
    "# 2. Ave error\n",
    "# 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save models and alpha\n",
    "import pickle\n",
    "with open('models/models.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)\n",
    "with open('models/alpha.pkl', 'wb') as f:\n",
    "    pickle.dump(alpha, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load models and alpha\n",
    "# import pickle\n",
    "# with open('models/models.pkl', 'rb') as f:\n",
    "#     models = pickle.load(f)\n",
    "\n",
    "# with open('models/alpha.pkl', 'rb') as f:\n",
    "#     alpha = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(x, models):\n",
    "    pred = []\n",
    "    for i in range(len(models)):\n",
    "        pred.append(models[i].predict(x))\n",
    "    pred = np.array(pred)\n",
    "    u, c = np.unique(pred, return_counts=True)\n",
    "    f_pred = u[np.argmax(c)]\n",
    "    return f_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8461538461538461\n",
      "Confusion Matrix: \n",
      " [[215  30]\n",
      " [ 46 203]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85       245\n",
      "           1       0.87      0.82      0.84       249\n",
      "\n",
      "    accuracy                           0.85       494\n",
      "   macro avg       0.85      0.85      0.85       494\n",
      "weighted avg       0.85      0.85      0.85       494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_n = len(X_test)\n",
    "Y_pred = np.zeros(test_n)\n",
    "for i in range(test_n):\n",
    "    x = X_test.iloc[i].values.reshape(1, -1)\n",
    "    temp = []\n",
    "    for m in range(len(models)):\n",
    "        temp.append(get_pred(x, models[m]))\n",
    "\n",
    "    u, c = np.unique(temp, return_counts=True)\n",
    "    Y_pred[i] = u[np.argmax(c)]\n",
    "\n",
    "print_report(y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unbiased Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/X_Unbiased.csv')\n",
    "y = pd.read_csv('data/Y_Unbiased.csv')\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Regression Accuracy:  0.5841784989858012\n",
      "KNN Accuracy:  0.7941176470588235\n",
      "Decision Tree Accuracy:  0.7317444219066938\n",
      "MLP Accuracy:  0.5821501014198783\n",
      "SVM Accuracy:  0.8017241379310345\n",
      "Random Forest Accuracy:  0.7616632860040567\n",
      "AdaBoost Accuracy:  0.8950304259634888\n",
      "Iteration:  1 Weighted Error:  0.26419878296146093\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5836713995943205\n",
      "KNN Accuracy:  0.8002028397565923\n",
      "Decision Tree Accuracy:  0.72920892494929\n",
      "MLP Accuracy:  0.5456389452332657\n",
      "SVM Accuracy:  0.7865111561866126\n",
      "Random Forest Accuracy:  0.7728194726166329\n",
      "AdaBoost Accuracy:  0.8975659229208925\n",
      "Iteration:  2 Weighted Error:  0.31192209346807415\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5851926977687627\n",
      "KNN Accuracy:  0.8017241379310345\n",
      "Decision Tree Accuracy:  0.72920892494929\n",
      "MLP Accuracy:  0.5938133874239351\n",
      "SVM Accuracy:  0.8047667342799188\n",
      "Random Forest Accuracy:  0.7981744421906694\n",
      "AdaBoost Accuracy:  0.8899594320486816\n",
      "Iteration:  3 Weighted Error:  0.3330644149875073\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5735294117647058\n",
      "KNN Accuracy:  0.7910750507099391\n",
      "Decision Tree Accuracy:  0.6825557809330629\n",
      "MLP Accuracy:  0.5806288032454361\n",
      "SVM Accuracy:  0.7966531440162272\n",
      "Random Forest Accuracy:  0.7829614604462475\n",
      "AdaBoost Accuracy:  0.8899594320486816\n",
      "Iteration:  4 Weighted Error:  0.3673339651835943\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5912778904665315\n",
      "KNN Accuracy:  0.7748478701825557\n",
      "Decision Tree Accuracy:  0.640973630831643\n",
      "MLP Accuracy:  0.5664300202839757\n",
      "SVM Accuracy:  0.7834685598377282\n",
      "Random Forest Accuracy:  0.7276876267748479\n",
      "AdaBoost Accuracy:  0.8848884381338742\n",
      "Iteration:  5 Weighted Error:  0.3970835964021597\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5826572008113591\n",
      "KNN Accuracy:  0.7971602434077079\n",
      "Decision Tree Accuracy:  0.6926977687626775\n",
      "MLP Accuracy:  0.5735294117647058\n",
      "SVM Accuracy:  0.7920892494929006\n",
      "Random Forest Accuracy:  0.7586206896551724\n",
      "AdaBoost Accuracy:  0.9056795131845842\n",
      "Iteration:  6 Weighted Error:  0.38994644669004863\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.6029411764705882\n",
      "KNN Accuracy:  0.789553752535497\n",
      "Decision Tree Accuracy:  0.72920892494929\n",
      "MLP Accuracy:  0.5228194726166329\n",
      "SVM Accuracy:  0.802738336713996\n",
      "Random Forest Accuracy:  0.736815415821501\n",
      "AdaBoost Accuracy:  0.8970588235294118\n",
      "Iteration:  7 Weighted Error:  0.40373974980207\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5948275862068966\n",
      "KNN Accuracy:  0.789553752535497\n",
      "Decision Tree Accuracy:  0.6825557809330629\n",
      "MLP Accuracy:  0.5826572008113591\n",
      "SVM Accuracy:  0.7961460446247465\n",
      "Random Forest Accuracy:  0.7738336713995944\n",
      "AdaBoost Accuracy:  0.8894523326572008\n",
      "Iteration:  8 Weighted Error:  0.42399824465304103\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5856997971602435\n",
      "KNN Accuracy:  0.8042596348884381\n",
      "Decision Tree Accuracy:  0.723630831643002\n",
      "MLP Accuracy:  0.577079107505071\n",
      "SVM Accuracy:  0.8022312373225152\n",
      "Random Forest Accuracy:  0.7652129817444219\n",
      "AdaBoost Accuracy:  0.9016227180527383\n",
      "Iteration:  9 Weighted Error:  0.42922620113003784\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5425963488843814\n",
      "KNN Accuracy:  0.7936105476673428\n",
      "Decision Tree Accuracy:  0.6404665314401623\n",
      "MLP Accuracy:  0.5927991886409736\n",
      "SVM Accuracy:  0.7956389452332657\n",
      "Random Forest Accuracy:  0.7545638945233266\n",
      "AdaBoost Accuracy:  0.8990872210953347\n",
      "Iteration:  10 Weighted Error:  0.431856604803422\n",
      "\n",
      "\n",
      "\n",
      "Login Regression Accuracy:  0.5841784989858012\n",
      "KNN Accuracy:  0.8164300202839757\n",
      "Decision Tree Accuracy:  0.72920892494929\n",
      "MLP Accuracy:  0.5730223123732252\n",
      "SVM Accuracy:  0.7971602434077079\n",
      "Random Forest Accuracy:  0.8002028397565923\n",
      "AdaBoost Accuracy:  0.9193711967545639\n",
      "Iteration:  11 Weighted Error:  0.45421092957363446\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iter = 11\n",
    "n = len(X_train)\n",
    "samples_len = n//2\n",
    "w = np.full(n, (1/n))\n",
    "models = []\n",
    "alpha = []\n",
    "n1 = 1/n\n",
    "\n",
    "for i in range(iter):\n",
    "    model1, error1, w1 = create_model(LogisticRegression(C=0.1, max_iter=10, penalty='l2'), X_train, y_train, w, samples_len)\n",
    "    model2, error2, w2 = create_model(KNeighborsClassifier(algorithm='auto', n_neighbors=3, weights='uniform'), X_train, y_train, w, samples_len)\n",
    "    model3, error3, w3 = create_model(DecisionTreeClassifier(criterion='entropy', max_depth=2, splitter='best'), X_train, y_train, w, samples_len)\n",
    "    model4, error4, w4 = create_model(MLPClassifier(activation='tanh', alpha=0.05, hidden_layer_sizes=(20, 30, 30, 20), learning_rate='adaptive', solver='adam'), X_train, y_train, w, samples_len)\n",
    "    model5, error5, w5 = create_model(SVC(C=10, gamma=0.1, kernel='linear'), X_train, y_train, w, samples_len)\n",
    "    model6, error6, w6 = create_model(RandomForestClassifier(criterion='entropy', max_depth=2, n_estimators=10), X_train, y_train, w, samples_len)\n",
    "    model7, error7, w7 = create_model(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=50), X_train, y_train, w, samples_len)\n",
    "\n",
    "    error = error1 + error2 + error3 + error4 + error5 + error6 + error7\n",
    "    error = error/7\n",
    "    alpha_round = 0.5 * np.log((1-error)/error)\n",
    "    w = w1 + w2 + w3 + w4 + w5 + w6 + w7\n",
    "    w = w/sum(w)\n",
    "\n",
    "\n",
    "    models.append([model1, model2, model3, model4, model5, model6, model7])\n",
    "    print('Login Regression Accuracy: ', accuracy_score(y_train, model1.predict(X_train)))\n",
    "    print('KNN Accuracy: ', accuracy_score(y_train, model2.predict(X_train)))\n",
    "    print('Decision Tree Accuracy: ', accuracy_score(y_train, model3.predict(X_train)))\n",
    "    print('MLP Accuracy: ', accuracy_score(y_train, model4.predict(X_train)))\n",
    "    print('SVM Accuracy: ', accuracy_score(y_train, model5.predict(X_train)))\n",
    "    print('Random Forest Accuracy: ', accuracy_score(y_train, model6.predict(X_train)))\n",
    "    print('AdaBoost Accuracy: ', accuracy_score(y_train, model7.predict(X_train)))\n",
    "    print('Iteration: ', i+1, 'Weighted Error: ', error)\n",
    "    print('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save models and alpha\n",
    "import pickle\n",
    "with open('models/models_unbiased.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)\n",
    "with open('models/alpha_unbiased.pkl', 'wb') as f:\n",
    "    pickle.dump(alpha, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8340080971659919\n",
      "Confusion Matrix: \n",
      " [[214  31]\n",
      " [ 51 198]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84       245\n",
      "           1       0.86      0.80      0.83       249\n",
      "\n",
      "    accuracy                           0.83       494\n",
      "   macro avg       0.84      0.83      0.83       494\n",
      "weighted avg       0.84      0.83      0.83       494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_n = len(X_test)\n",
    "Y_pred = np.zeros(test_n)\n",
    "for i in range(test_n):\n",
    "    x = X_test.iloc[i].values.reshape(1, -1)\n",
    "    temp = []\n",
    "    for m in range(len(models)):\n",
    "        temp.append(get_pred(x, models[m]))\n",
    "\n",
    "    u, c = np.unique(temp, return_counts=True)\n",
    "    Y_pred[i] = u[np.argmax(c)]\n",
    "\n",
    "print_report(y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
